{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 6 Project: Book Recommender description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is group 6 project's description. The description includes two parts, which are model part and app part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model part\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `model.py` Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model class contains three different types of recommender. Using item-based collaborative filtering, a model will look for books that have high correlations with the user's input according to book titles, and ratings from other users. Then it will generate the top five books that have the highest similarity. Second recommender is called Content-Based Collaborative Filtering. This system tries to generate the recommendation based on multiple features. Once the user input the book that he/she likes, we generate the feature vector including Title, Author, Publisher and genre. Then calculate the element-wise squared distance between input vectors with other books’ feature vectors. Then the top 5 with the closest distance book will be recommended. Last model is a mixed model which combines the first two models. The mixed model will first compare the features vectors including genre and author than compare the summary’s cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `model_1.py` Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model gets the top N most similar books based on their summary using cosine similarity. The model is broken up into two parts model_1 and model_Summary_preprocessing. The model was split up in this way to reduce runtime and memory requirements for AWS. Model_Summary_preprocessing generates the files necessary to run the cosine similarity which is a 2Dvector of the similarity of each ngram.  For us we only run this code once since this vector generation can take the upwards of an hour. Then we have model_1 which is deployed on AWS. This takes the two files generated by model_Summary_preprocessing, computes cosine similarity and returns the top n most similar books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## App part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `app.py` Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The front end is built using HTML and bootstrap. There are several major components such as overview, explore more, find a book and about us. For core machine learning part, we added extra authentication just to make sure the instance won’t crash. Our whole project is deployed on EC2 and documented on GitHub under flaskapp folder."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b629c3126b5df0b3c19ac5f524890cb3a3a2e86c1a2f2c4b1c29287aa73e65d0"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
